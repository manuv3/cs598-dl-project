{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkC1RqNMYvwd"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "We need to import and transform following MIMIC-III datasets:\n",
        "DIAGNOSES_ICD.csv: Each row in this file maps Hospitalization ID (HADM_ID) of a patient with a unique ICD_9 CODE.\n",
        "  \n",
        "  Example:\n",
        "  \n",
        "  |ROW_ID|SUBJECT_ID|**HADM_ID**|SEQ_NUM|**ICD9_CODE**|\n",
        "  |--------|------------|---------|---------|-----------|\n",
        "  |1297|109|172335|1|\"0030\"|\n",
        "  |1299|109|172335|3|\"0038\"|\n",
        "  |1301|109|173633|5|\"0031\"|\n",
        "  \n",
        "  \n",
        "  The aim is to transform this dataset to dataframe DIGNOSES with index as HADM_ID and columns as unique ICD_9 codes (6984 in total), to represent multi-hot encoding of ICD_9 codes for given hospitalization.\n",
        "  \n",
        "  |HADM_ID|ICD9_CODE_0030|ICD9_CODE_0031|ICD9_CODE_0038|\n",
        "  |-------|--------------|--------------|--------------|\n",
        "  |172335|1|0|1|\n",
        "  |173633|0|1|0|\n",
        "  \n",
        "  \n",
        "  \n",
        "  NOTEEVENTS.csv: Each row maps HADM_ID (Hospitalization ID) with a free text Discharge summary (TEXT) field.\n",
        "  \n",
        "  |ROW_ID|SUBJECT_ID|**HADM_ID**|CHARTDATE|CHARTTIME|STORETIME|CATEGORY|DESCRIPTION|CGID|ISERROR|**TEXT**|\n",
        "  |------|----------|-----------|---------|---------|---------|--------|-----------|----|-------|--------|\n",
        "  |174|22532|167853|2151-08-04|||Discharge summary|Report|||Admission Date:  [\\*\\*2151-7-16**]       Discharge Date:  [\\*\\*2151-8-4**] Service: ADDENDUM: RADIOLOGIC STUDIES:  Radiologic studies also included a chest| \n",
        " \n",
        "  The aim is to transform this to dataframe with HADM_ID as index and TEXT as column.\n",
        "\n",
        "  |HADM_ID|TEXT|\n",
        "  |-------|----|\n",
        "  |167853|Admission Date:  [\\*\\*2151-7-16**]       Discharge Date:  [\\*\\*2151-8-4**] Service: ADDENDUM: RADIOLOGIC STUDIES:  Radiologic studies also included a chest|\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "The processed dataframes are then stored (as csv) for further usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fweFyNRw00So"
      },
      "source": [
        "## Critical Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUagBPMOuMAy",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Critical imports\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq1zfJttrKoX"
      },
      "outputs": [],
      "source": [
        "# Mount the project directory in Google drive. (Its only intended to be run in colab environment.)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7ixu37prKoX"
      },
      "outputs": [],
      "source": [
        "# Define the base project directory.\n",
        "\n",
        "PROJECT_DIR = 'drive/My Drive/cs598-dl/' # For Google drive only\n",
        "\n",
        "# PROJECT_DIR = '../' # For local directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm0r3kqTrKoY"
      },
      "outputs": [],
      "source": [
        "'''We first process MIMIC-III DIAGNOSES_ICD dataset.'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sta = time.time()"
      ],
      "metadata": {
        "id": "FlY7NMX0tGhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs4q87VD8GPA",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Read DIAGNOSES_ICD.csv from data directory, and pre-process.\n",
        "\n",
        "diagnoses_df = pd.read_csv(PROJECT_DIR + 'data/DIAGNOSES_ICD.csv', usecols=['HADM_ID', 'ICD9_CODE'])\n",
        "diagnoses_df = diagnoses_df.astype({'ICD9_CODE': 'string'})\n",
        "\n",
        "# Collect all unique ICD_9 codes and create new DataFrame codes_df.\n",
        "codes_df = pd.DataFrame(diagnoses_df['ICD9_CODE'].unique(), columns = ['ICD9_CODE'])\n",
        "\n",
        "# Create DataFrame representing one-hot encoding of ICD_9 codes.\n",
        "one_hot_enc_df = pd.get_dummies(codes_df, columns = ['ICD9_CODE'], dtype='bool')\n",
        "\n",
        "# Join codes_df and one_hot_enc_df, based on index\n",
        "codes_df = codes_df.join(one_hot_enc_df)\n",
        "\n",
        "# Merge diagnoses_df and codes_df based on column 'ICD9_CODE'. \n",
        "# Dask operations are lazy and do not materialize until 'compute()' method is invoked.\n",
        "diagnoses_df = diagnoses_df.merge(codes_df, on='ICD9_CODE')\n",
        "\n",
        "diagnoses_df = diagnoses_df.drop(['ICD9_CODE'], axis = 1)\n",
        "\n",
        "# This step will group all ICD_9 codes corresponding to a given HADM_ID and build a multi-hot embedding.\n",
        "diagnoses_df = diagnoses_df.groupby('HADM_ID').any().reset_index()\n",
        "\n",
        "print(diagnoses_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiM5UPsjrKoZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Read DIAGNOSES_ICD.csv from data directory, and pre-process.\n",
        "\n",
        "diagnoses_df = pd.read_csv(PROJECT_DIR + 'data/DIAGNOSES_ICD.csv', usecols=['HADM_ID', 'ICD9_CODE'])\n",
        "diagnoses_df = diagnoses_df.astype({'ICD9_CODE': 'string'})\n",
        "\n",
        "# Collect all unique ICD_9 codes and create new DataFrame codes_df.\n",
        "codes_df = pd.DataFrame(diagnoses_df['ICD9_CODE'].unique(), columns = ['ICD9_CODE'])\n",
        "\n",
        "# Create DataFrame representing one-hot encoding of ICD_9 codes.\n",
        "one_hot_enc_df = pd.get_dummies(codes_df, columns = ['ICD9_CODE'], dtype='bool')\n",
        "\n",
        "# Join codes_df and one_hot_enc_df, based on index\n",
        "codes_df = codes_df.join(one_hot_enc_df)\n",
        "\n",
        "# Merge diagnoses_df and codes_df based on column 'ICD9_CODE'. \n",
        "diagnoses_df = diagnoses_df.merge(codes_df, on='ICD9_CODE')\n",
        "diagnoses_df = diagnoses_df.drop(['ICD9_CODE'], axis = 1)\n",
        "\n",
        "# This step will group all ICD_9 codes corresponding to a given HADM_ID and build a multi-hot embedding.\n",
        "diagnoses_df = diagnoses_df.groupby('HADM_ID').any().reset_index()\n",
        "\n",
        "print(diagnoses_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4braT0drKoa"
      },
      "outputs": [],
      "source": [
        "'''Next we process MIMIC-III NOTEEVENTS dataset.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHS81ejCQulU"
      },
      "outputs": [],
      "source": [
        "# Import dataset and pre-process.\n",
        "notes_df = pd.read_csv('drive/My Drive/cs598-dl/data/NOTEEVENTS.csv', usecols=['HADM_ID', \"CATEGORY\",\"DESCRIPTION\", \"TEXT\"])\n",
        "notes_df = notes_df.dropna()\n",
        "\n",
        "# Only filter-in notes which are 'Discharge summary' and are of sub-type 'Report'.\n",
        "notes_df = notes_df[(notes_df['CATEGORY'] == 'Discharge summary') & (notes_df['DESCRIPTION'] == 'Report')]\n",
        "notes_df = notes_df.drop(['CATEGORY', 'DESCRIPTION'], axis=1)\n",
        "notes_df = notes_df.astype({'HADM_ID': 'int64'})\n",
        "notes_df = notes_df.drop_duplicates(subset = 'HADM_ID')\n",
        "print(notes_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKacrf5GrKob"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "We next select the subset of rows in diagnoses_df and notes_df with common set of HADM_IDs, \n",
        "and remove other rows from each DataFrame. Such rows can not be used in training or testing.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcpXLsjZad0K"
      },
      "outputs": [],
      "source": [
        "# Collect all hadm_ids from diagnoses_df\n",
        "hadm_ids_from_diagnoses_df = diagnoses_df.filter(items = ['HADM_ID'])\n",
        "\n",
        "# Collect all hadm_ids from diagnoses_df\n",
        "hadm_ids_from_notes_df = notes_df.filter(items = ['HADM_ID'])\n",
        "\n",
        "# Generate DataFrame with common set of HADM_IDs.\n",
        "hadm_ids_df = hadm_ids_from_diagnoses_df.merge(hadm_ids_from_notes_df, how = 'inner')\n",
        "\n",
        "# Filter rows in daignoses_df by merging with DataFrame containing common HADM_IDs.\n",
        "diagnoses_df = diagnoses_df.merge(hadm_ids_df, on='HADM_ID', how = 'right')\n",
        "\n",
        "# Similarly, filter rows in notes_df by merging with DataFrame containing common HADM_IDs.\n",
        "notes_df = notes_df.merge(hadm_ids_df, on='HADM_ID', how = 'inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUKIuqsLvCvz"
      },
      "outputs": [],
      "source": [
        "# Pickle diagnoses_df\n",
        "diagnoses_df.to_pickle(PROJECT_DIR + 'data/DIAGNOSES.pkl')\n",
        "\n",
        "# Pickle notes_df\n",
        "notes_df.to_pickle(PROJECT_DIR + 'data/NOTES.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end = time.time()\n",
        "print('Time taken in data processing:' + str(end - sta))"
      ],
      "metadata": {
        "id": "zGsD-Ij_tYZM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:.conda-rapids] *",
      "language": "python",
      "name": "conda-env-.conda-rapids-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}