{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dO4FnbVjYluf",
      "metadata": {
        "id": "dO4FnbVjYluf"
      },
      "source": [
        "# Validate Pre-Trained DL Models\n",
        "\n",
        "This notebook loads the saved-state for all 4 models and validates them:\n",
        "\n",
        "- main model (CNN + D2V)\n",
        "- cnn-only model\n",
        "- d2v-only model\n",
        "- cnn-attention model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4599f50e",
      "metadata": {
        "id": "4599f50e"
      },
      "outputs": [],
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import math\n",
        "import csv\n",
        "import pickle\n",
        "import time\n",
        "import bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de8964d2",
      "metadata": {
        "id": "de8964d2"
      },
      "outputs": [],
      "source": [
        "# Set seed for random generators\n",
        "\n",
        "seed = 24\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3924be7d",
      "metadata": {
        "id": "3924be7d"
      },
      "outputs": [],
      "source": [
        "# Set the device type as cpu or cuda depending upon the execution environment.\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s6DKWE5tho7U",
      "metadata": {
        "id": "s6DKWE5tho7U"
      },
      "outputs": [],
      "source": [
        "# Clone git repo containing all the pre-processed data files. THIS IS ONLY NEEDED IN CLOUD ENVIRONMENT.\n",
        "\n",
        "!git clone https://github.com/manuv3/cs598-dl-project.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fVmreShG6XXZ",
      "metadata": {
        "id": "fVmreShG6XXZ"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = './cs598-dl-project/data'\n",
        "\n",
        "# USE BELOW FOR LOCAL TESTING\n",
        "# DATA_DIR = '../data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66ecd213",
      "metadata": {
        "id": "66ecd213"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Import the \"codes\" DataFrame, with HADM_ID (Hospitalization ID) as index and multi-hot encoding of \n",
        "ICD9-codes (booleans) as columns. Its dimensions are a 52691 rows Ã— 6984 columns. So, we have 6984 ICD9 codes.\n",
        "'''\n",
        "\n",
        "codes = pd.read_pickle(DATA_DIR + '/diagnoses.pkl.gz')\n",
        "codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "576c6d78",
      "metadata": {
        "id": "576c6d78"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Split the HADM_IDs (hospitalization IDs) into train-test in 90:10 ratio. Two lists are generated:\n",
        "    - hadm_ids_train\n",
        "    - hadm_ids_test\n",
        "'''\n",
        "\n",
        "hadm_ids_train, hadm_ids_test = train_test_split(codes.index.tolist(), test_size = 0.10, random_state=seed)\n",
        "print(hadm_ids_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e1de3f6",
      "metadata": {
        "id": "5e1de3f6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Load the Doc2Vec embeddings for discharge summary reports, generated during pre-processing step. The data is in \n",
        "Gensim KeyedVector format.\n",
        "'''\n",
        "\n",
        "dv = KeyedVectors.load(DATA_DIR + '/dv.kv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d71e938",
      "metadata": {
        "id": "2d71e938"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Load the Word2Vec embeddings of all the words in vocabulary of the whole corpus, generated during the \n",
        "pre-processing step. This data is in Gensim KeyedVector format.\n",
        "'''\n",
        "\n",
        "wv = KeyedVectors.load(DATA_DIR + '/wv.kv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KyuZNasTQ_Dm",
      "metadata": {
        "id": "KyuZNasTQ_Dm"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Load the dictionary mapping HADM_ID with a tokenized document. These tokens are basically the list of words \n",
        "belonging to the corresponding Discharge summary report.\n",
        "'''\n",
        "\n",
        "with bz2.open(DATA_DIR + '/tokens_map.pkl.bz2', 'rb') as handle:\n",
        "  tokens_dict = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8b7f611",
      "metadata": {
        "id": "d8b7f611"
      },
      "outputs": [],
      "source": [
        "# how many samples per batch to load\n",
        "batch_size = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "710a0882",
      "metadata": {
        "id": "710a0882"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Define the implementation of PyTorch Dataset. This is very light-weight, as all the data tensors (dv_train, \n",
        "dv_test, tokens_train, tokens_test, and codes_train) have already been pushed to GPU memory. So they can be \n",
        "referenced by HADM_ID index. This dataset simply returns the input index as the data in __getitem()__ method, which \n",
        "will be used during model training to access the input and output data from data tensors.\n",
        "'''\n",
        "\n",
        "class DocumentsDataset(Dataset):\n",
        "    def __init__(self, hadm_ids):\n",
        "        super(DocumentsDataset).__init__()\n",
        "        self.hadm_ids = hadm_ids\n",
        "    def __len__(self):\n",
        "        return len(self.hadm_ids)\n",
        "    def __getitem__(self, idx):\n",
        "        hadm_id = self.hadm_ids[idx]\n",
        "        tokens = tokens_dict[hadm_id]\n",
        "        word_vecs = torch.Tensor(wv.__getitem__(tokens))\n",
        "        x_cnn = torch.zeros((700, 100))\n",
        "        x_cnn[0:word_vecs.shape[0]] = word_vecs\n",
        "        x_d2v = torch.Tensor(dv[str(hadm_id)].tolist())\n",
        "        y = torch.tensor(codes.loc[hadm_id].to_numpy())\n",
        "        return x_d2v, x_cnn, y "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d476af18",
      "metadata": {
        "id": "d476af18"
      },
      "outputs": [],
      "source": [
        "# prepare test dataloader\n",
        "test_loader = DataLoader(DocumentsDataset(hadm_ids_test), batch_size = batch_size)\n",
        "\n",
        "print(\"# of val batches:\", len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "772d57a5",
      "metadata": {
        "id": "772d57a5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Define the model class which represents the D2V sub-component that uses single fully connected layer to fine-tune \n",
        "the Doc2Vec embeddings. This representation is passed through a Dropout layer to perform regularization, and the output is the \n",
        "vector representing the input document.\n",
        "'''\n",
        "\n",
        "class D2V(nn.Module):\n",
        "    def __init__(self, dropout = 0.20):\n",
        "        super(D2V, self).__init__()\n",
        "        self.fc = nn.Linear(128, 64)\n",
        "        self.dropout = nn.Dropout(p = dropout)\n",
        "    def forward(self, x):\n",
        "        y = self.dropout(F.relu(self.fc(x)))\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbede23a",
      "metadata": {
        "id": "dbede23a"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Define the model class which is the building block of the CNN sub-component containining a single layer of \n",
        "multi-channel CNN kernel, activation function, and max-pooling layer.\n",
        "'''\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, kernel_size, in_channels = 1, out_channels = 64):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, (kernel_size, 100))\n",
        "        self.pool = nn.MaxPool2d((700 - kernel_size + 1,1))\n",
        "    def forward(self, x):\n",
        "        y = self.pool(F.relu(self.conv(x))).squeeze()\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdae0791",
      "metadata": {
        "id": "fdae0791"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Define the model class for CNN sub-component. This component concatenates the output of 3 CNN components (described \n",
        "previously), each corresponding to different kernel size (3, 4, and 5 words). Its input is \"concatenated\" Word2Vec \n",
        "embeddings of all words within a document, which it passes to the three CNN components parallely, and then combines \n",
        "their output. This representation is passed through a Dropout layer to perform regularization, and the output is the \n",
        "vector representing the input document.\n",
        "'''\n",
        "\n",
        "class CNN_COMBINED(nn.Module):\n",
        "    def __init__(self, dropout = 0.20):\n",
        "        super(CNN_COMBINED, self).__init__()\n",
        "        self.conv_3 = CNN(3)\n",
        "        self.conv_4 = CNN(4)\n",
        "        self.conv_5 = CNN(5)\n",
        "        self.dropout = nn.Dropout(p = dropout)\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        return self.dropout(torch.cat((self.conv_3(x), self.conv_4(x), self.conv_5(x)), dim = 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88334e11",
      "metadata": {
        "id": "88334e11"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Define the class for the main model, which uses both from D2V and CNN_COMBINED components, in the Encoder layer.\n",
        "'''\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, dropout_d2v, dropout_cnn):\n",
        "        super(Net, self).__init__()\n",
        "        self.d2v = D2V(dropout = dropout_d2v)\n",
        "        self.cnn = CNN_COMBINED(dropout = dropout_cnn)\n",
        "        self.fc2 = nn.Linear(256, 6984)\n",
        "\n",
        "    def forward(self, x_d2v, x_cnn):\n",
        "        y_d2v = self.d2v(x_d2v)\n",
        "        y_cnn = self.cnn(x_cnn)\n",
        "        y = torch.sigmoid(self.fc2(torch.cat((y_d2v, y_cnn), dim = 1)))\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbd300fb",
      "metadata": {
        "id": "fbd300fb"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Define the class for the cnn-only ablation model, which uses only the CNN_COMBINED component in the Encoder layer. \n",
        "'''\n",
        "\n",
        "class Net_CNN(nn.Module):\n",
        "    def __init__(self, dropout):\n",
        "        super(Net_CNN, self).__init__()\n",
        "        self.cnn = CNN_COMBINED(dropout)\n",
        "        self.fc2 = nn.Linear(192, 6984)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.fc2(self.cnn(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qGYv4g1tSPHZ",
      "metadata": {
        "id": "qGYv4g1tSPHZ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Define the class for the d2v-only ablation model, which uses only the D2V component in the Encoder layer. \n",
        "'''\n",
        "\n",
        "class Net_D2V(nn.Module):\n",
        "    def __init__(self, dropout):\n",
        "        super(Net_D2V, self).__init__()\n",
        "        self.d2v = D2V(dropout = dropout)\n",
        "        self.fc = nn.Linear(64, 6984)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.fc(self.d2v(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z1B3AK30D9vl",
      "metadata": {
        "id": "Z1B3AK30D9vl"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Define the class for the cnn + attention model.\n",
        "'''\n",
        "\n",
        "class Net_CNN_Attention(nn.Module):\n",
        "    def __init__(self, kernel_size, dropout):\n",
        "        super(Net_CNN_Attention, self).__init__()\n",
        "        self.conv = nn.Conv1d(100, 50, kernel_size)\n",
        "        self.att = nn.Linear(50, 6984, bias = False)\n",
        "        self.dropout = nn.Dropout(p = dropout)\n",
        "        self.fc = nn.Linear(50, 1)\n",
        "    def forward(self, x):\n",
        "        x = x.permute((0, 2, 1))\n",
        "        y = torch.relu(self.conv(x))\n",
        "        # apply attention\n",
        "        alpha = F.softmax(self.att.weight.matmul(y), dim=2)\n",
        "        # compute output by matrix multiplication of attention matrix with internal state\n",
        "        y = alpha.matmul(y.permute(0, 2, 1))\n",
        "        y = self.dropout(y)\n",
        "        y = torch.sigmoid(self.fc(y)).squeeze()\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5882723c",
      "metadata": {
        "id": "5882723c"
      },
      "outputs": [],
      "source": [
        "# Model Evaluation\n",
        "\n",
        "from sklearn.metrics import *\n",
        "\n",
        "\n",
        "def classification_metrics(Y_pred, Y_true):\n",
        "    \"\"\"\n",
        "    Calculate peformance metrics using scikit-learn.\n",
        "    \n",
        "    Arguments:\n",
        "        Y_pred: Long dtype Tensor of output values for the test set batch, as predicted by model.\n",
        "        Y_true: Long dtype Tensor of true values in the test-set batch.\n",
        "        \n",
        "    Outputs:\n",
        "        precision: overall micro-averaged precision score\n",
        "        recall: overall micro-averaged recall score\n",
        "        f1: overall micro-averaged f1 score\n",
        "        \n",
        "    REFERENCE: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
        "\"\"\"\n",
        "    \n",
        "    precision, recall, f1score = precision_score(Y_true, Y_pred, average = 'micro'), \\\n",
        "                                           recall_score(Y_true, Y_pred, average = 'micro'), \\\n",
        "                                           f1_score(Y_true, Y_pred, average = 'micro')\n",
        "    return precision, recall, f1score\n",
        "\n",
        "\n",
        "def evaluate(model, loader, threshold, only_cnn = False, only_d2v = False):\n",
        "    \"\"\"\n",
        "    Evaluate the model.\n",
        "    \n",
        "    Arguments:\n",
        "        model: Trained model of type nn.Module\n",
        "        loader: Test DataLoader\n",
        "        \n",
        "    Outputs:\n",
        "        precision: overall micro-averaged precision score\n",
        "        recall: overall micro-averaged recall score\n",
        "        f1: overall micro-averaged f1 score\n",
        "    \"\"\"\n",
        "    \n",
        "    model.eval()\n",
        "    all_y_true = torch.LongTensor()\n",
        "    all_y_pred = torch.LongTensor()\n",
        "    for x_d2v,x_cnn, y in loader:\n",
        "        if not only_cnn:\n",
        "          x_d2v = x_d2v.to(device)\n",
        "        if not only_d2v:\n",
        "          x_cnn = x_cnn.to(device)\n",
        "        if only_d2v:\n",
        "          y_hat = model(x_d2v)\n",
        "        elif only_cnn:\n",
        "          y_hat = model(x_cnn)\n",
        "        else:\n",
        "          y_hat = model(x_d2v, x_cnn)\n",
        "        y_pred = y_hat.detach().to('cpu').apply_(lambda x: 1 if x > threshold else 0)\n",
        "        all_y_true = torch.cat((all_y_true, y.long()), dim=0)\n",
        "        all_y_pred = torch.cat((all_y_pred,  y_pred.to('cpu').long()), dim=0)\n",
        "        \n",
        "    precision, recall, f1 = classification_metrics(all_y_pred.detach().numpy(), all_y_true.detach().numpy())\n",
        "    print(f\"precision: {precision:.3f}, recall: {recall:.3f}, f1: {f1:.3f}\")\n",
        "    return precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mH7ZsUOgESSQ",
      "metadata": {
        "id": "mH7ZsUOgESSQ"
      },
      "outputs": [],
      "source": [
        "# Utility method to load pre-trained model state\n",
        "\n",
        "    \n",
        "def load_checkpoint(model, checkpoint_file):\n",
        "    model.load_state_dict(torch.load('./cs598-dl-project/trained_models/' + checkpoint_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0fd1e16",
      "metadata": {
        "id": "d0fd1e16"
      },
      "source": [
        "## Evaluate pre-trained main model\n",
        "\n",
        "### Hyperparameters\n",
        "\n",
        "|Hyperparameter|Value|\n",
        "|:-------------|:----| \n",
        "|CLASSIFICATION_THRESHOLD|0.20|\n",
        "|DROPOUT_D2V|0.30|\n",
        "|DROPOUT_CNN|0.20|\n",
        "\n",
        "    This model was trained for 20 epochs. \n",
        "    The final model was selected with state from 19th epoch.\n",
        "    Time spent in training and evaluation: 1236.32 seconds.\n",
        "\n",
        "### Results\n",
        "\n",
        "After the end of run, the results should be similar to below table:\n",
        "\n",
        "|Precision|Recall|F1-score|\n",
        "|:--------|:-----|:-------|\n",
        "|0.502|0.351|0.413|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9060b9f",
      "metadata": {
        "id": "d9060b9f"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Evaluate pre-trained main model\n",
        "'''\n",
        "\n",
        "# Hyperparameters\n",
        "CLASSIFICATION_THRESHOLD = 0.20\n",
        "DROPOUT_D2V = 0.30\n",
        "DROPOUT_CNN = 0.20\n",
        "\n",
        "# Define the model\n",
        "main_model = Net(DROPOUT_D2V, DROPOUT_CNN)\n",
        "\n",
        "# Load model state\n",
        "load_checkpoint(main_model, 'main_model.pth')\n",
        "\n",
        "# Move to GPU\n",
        "main_model.to(device)\n",
        "\n",
        "# Evaluate\n",
        "precision, recall, f1 = evaluate(main_model, test_loader, threshold = CLASSIFICATION_THRESHOLD)\n",
        "\n",
        "# Assert\n",
        "assert (round(precision, 3), round(recall, 3), round(f1, 3)) == (0.502, 0.351, 0.413)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "738b1582",
      "metadata": {
        "id": "738b1582"
      },
      "source": [
        "## Evaluate pre-trained cnn-only model (ablation)\n",
        "\n",
        "### Hyperparameters\n",
        "\n",
        "|Hyperparameter|Value|\n",
        "|:-------------|:----| \n",
        "|CLASSIFICATION_THRESHOLD|0.20|\n",
        "|DROPOUT_CNN|0.20|\n",
        "\n",
        "    This model was trained for 15 epochs. \n",
        "    The final model was selected with state from 13th epoch.\n",
        "    Time spent in training and evaluation: 1129.95 seconds.\n",
        "        \n",
        "### Results\n",
        "\n",
        "After the end of run, the results should be similar to below table:\n",
        "\n",
        "|Precision|Recall|F1-score|\n",
        "|:--------|:-----|:-------|\n",
        "|0.478|0.345|0.401|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "642abb8d",
      "metadata": {
        "id": "642abb8d"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Evaluate pre-trained cnn-only model\n",
        "'''\n",
        "\n",
        "# Hyperparameters\n",
        "CLASSIFICATION_THRESHOLD = 0.20\n",
        "DROPOUT_CNN = 0.20\n",
        "\n",
        "# Define the model\n",
        "cnn_only_model = Net_CNN(DROPOUT_CNN)\n",
        "\n",
        "# Load model state\n",
        "load_checkpoint(cnn_only_model, 'cnn_only_model.pth')\n",
        "\n",
        "# Move to GPU\n",
        "cnn_only_model.to(device)\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "precision, recall, f1 = evaluate(cnn_only_model, test_loader, threshold = CLASSIFICATION_THRESHOLD, only_cnn = True)\n",
        "\n",
        "# Assert\n",
        "assert (round(precision, 3), round(recall, 3), round(f1, 3)) == (0.478, 0.345, 0.401)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce23eb7",
      "metadata": {
        "id": "cce23eb7"
      },
      "source": [
        "## Evaluate pre-trained d2v-only model (ablation)\n",
        "\n",
        "### Hyperparameters\n",
        "\n",
        "|Hyperparameter|Value|\n",
        "|:-------------|:----| \n",
        "|CLASSIFICATION_THRESHOLD|0.18|\n",
        "|DROPOUT_D2V|0.10|\n",
        "\n",
        "    This model was trained for 20 epochs. \n",
        "    The final model was selected with state from 15th epoch.\n",
        "    Time spent in training and evaluation: 1188.31 seconds.\n",
        "        \n",
        "### Results\n",
        "\n",
        "After the end of run, the results should be similar to below table:\n",
        "\n",
        "|Precision|Recall|F1-score|\n",
        "|:--------|:-----|:-------|\n",
        "|0.350|0.261|0.299|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaec7737",
      "metadata": {
        "id": "eaec7737"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Evaluate pre-trained d2v-only model\n",
        "'''\n",
        "\n",
        "# Hyperparameters\n",
        "CLASSIFICATION_THRESHOLD = 0.18\n",
        "DROPOUT_D2V = 0.10\n",
        "\n",
        "# Define the model\n",
        "d2v_only_model = Net_D2V(DROPOUT_D2V)\n",
        "\n",
        "# Load model state\n",
        "load_checkpoint(d2v_only_model, 'd2v_only_model.pth')\n",
        "\n",
        "# Move to GPU\n",
        "d2v_only_model.to(device)\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "precision, recall, f1 = evaluate(d2v_only_model, test_loader, threshold = CLASSIFICATION_THRESHOLD, only_d2v = True)\n",
        "\n",
        "# Assert\n",
        "assert (round(precision, 3), round(recall, 3), round(f1, 3)) == (0.350, 0.261, 0.299)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48a725bb",
      "metadata": {
        "id": "48a725bb"
      },
      "source": [
        "## Evaluate pre-trained cnn-attention model (additional model)\n",
        "\n",
        "### Hyperparameters\n",
        "\n",
        "|Hyperparameter|Value|\n",
        "|:-------------|:----|\n",
        "|CLASSIFICATION_THRESHOLD|0.30|\n",
        "|DROPOUT_CNN|0.10|\n",
        "|KERNEL_SIZE|5|\n",
        "\n",
        "    This model was trained for 20 epochs. \n",
        "    The final model was selected with state from 19th epoch.\n",
        "    Time spent in training and evaluation: 2240.29 seconds.\n",
        "        \n",
        "### Results\n",
        "\n",
        "After the end of run, the results should be similar to below table:\n",
        "\n",
        "|Precision|Recall|F1-score|\n",
        "|:--------|:-----|:-------|\n",
        "|0.520|0.372|0.434|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305f5826",
      "metadata": {
        "id": "305f5826"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Evaluate pre-trained cnn-attention model\n",
        "'''\n",
        "\n",
        "# Hyperparameters\n",
        "CLASSIFICATION_THRESHOLD = 0.30\n",
        "DROPOUT_CNN = 0.10\n",
        "KERNEL_SIZE = 5\n",
        "\n",
        "# Define the model\n",
        "cnn_attention_model = Net_CNN_Attention(kernel_size = KERNEL_SIZE, dropout = DROPOUT_CNN)\n",
        "\n",
        "# Load model state\n",
        "load_checkpoint(cnn_attention_model, 'cnn_attention_model.pth')\n",
        "\n",
        "# Move to GPU\n",
        "cnn_attention_model.to(device)\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "precision, recall, f1 = evaluate(cnn_attention_model, test_loader, threshold = CLASSIFICATION_THRESHOLD, only_cnn = True)\n",
        "\n",
        "# Assert\n",
        "assert (round(precision, 3), round(recall, 3), round(f1, 3)) == (0.520, 0.372, 0.434)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "dl"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}