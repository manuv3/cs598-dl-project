{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dO4FnbVjYluf",
   "metadata": {
    "id": "dO4FnbVjYluf"
   },
   "source": [
    "# DL Model\n",
    "This notebook defines, trains, and validates the DL model.\n",
    "\n",
    "It first loads following data files generated from pre-processing steps. Input and output data Tensors are created and pushed to GPU memory. Then, model is defined, trained, and validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599f50e",
   "metadata": {
    "id": "4599f50e"
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8964d2",
   "metadata": {
    "id": "de8964d2"
   },
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3924be7d",
   "metadata": {
    "id": "3924be7d"
   },
   "outputs": [],
   "source": [
    "# Set the device type as cpu or cuda depending upon the execution environment.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a17522",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5a17522",
    "outputId": "103fcd5b-765e-48e5-b4f6-7bb7ae3e535d"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VU63T3_q2Q1U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VU63T3_q2Q1U",
    "outputId": "4c2dd7a4-3a3d-42ef-e036-746a22331c13"
   },
   "outputs": [],
   "source": [
    "# Mount the google drive at 'drive' directory in the colab virtual machine.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BkZW3gUB2f9y",
   "metadata": {
    "id": "BkZW3gUB2f9y"
   },
   "outputs": [],
   "source": [
    "# Define variable to point to the project directory in google drive.\n",
    "\n",
    "PROJECT_DIR = 'drive/My Drive/cs598-dl/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99523caa",
   "metadata": {
    "id": "99523caa"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Import the \"notes\" DataFrame, with HADM_ID (Hospitalization ID) as index and TEXT (Discharge summary) as column, \n",
    "created in pre-processing step. Its dimensions are 52691 rows × 1 columns.\n",
    "'''\n",
    "\n",
    "notes_df = pd.read_pickle(PROJECT_DIR + 'data/notes.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cda90a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "4cda90a0",
    "outputId": "8439d1cd-7c0e-456d-adaa-ae55207b035c"
   },
   "outputs": [],
   "source": [
    "notes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ecd213",
   "metadata": {
    "id": "66ecd213"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Import the \"codes\" DataFrame, with HADM_ID (Hospitalization ID) as index and multi-hot encoding of \n",
    "ICD9-codes (booleans) as columns. Its dimensions are a 52691 rows × 6984 columns. So, we have 6984 ICD9 codes.\n",
    "'''\n",
    "\n",
    "codes = pd.read_pickle(PROJECT_DIR + 'data/diagnoses.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3121bbbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "3121bbbb",
    "outputId": "525a23a8-0802-45d8-ca7d-79462caaacb5"
   },
   "outputs": [],
   "source": [
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c6d78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "576c6d78",
    "outputId": "8aaadaba-5c0f-4072-f0d1-0dffd310c8ff"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Split the HADM_IDs (hospitalization IDs) into train-test in 90:10 ratio. Two lists are generated:\n",
    "    - hadm_ids_train\n",
    "    - hadm_ids_test\n",
    "'''\n",
    "\n",
    "hadm_ids_train, hadm_ids_test = train_test_split(notes_df.index.tolist(), test_size = 0.10, random_state=seed)\n",
    "print(hadm_ids_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MNZBiZxh_dRR",
   "metadata": {
    "id": "MNZBiZxh_dRR"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create training tensor consisting of multi-hot ICD9-codes in each row, where each row corresponds to HADM_ID in \n",
    "hadm_ids_train list.\n",
    "'''\n",
    "\n",
    "codes_train = torch.zeros((len(hadm_ids_train), 6984), dtype=bool)\n",
    "for index, hadm_id in enumerate(hadm_ids_train):\n",
    "  vec = torch.tensor(codes.loc[hadm_id].to_numpy())\n",
    "  codes_train[index] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40F_id17AngY",
   "metadata": {
    "id": "40F_id17AngY"
   },
   "outputs": [],
   "source": [
    "# Push the tensor to GPU memory.\n",
    "\n",
    "codes_train = codes_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M6Jk-UGLAsqq",
   "metadata": {
    "id": "M6Jk-UGLAsqq"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create test tensor consisting of multi-hot ICD9-codes in each row, where each row corresponds to HADM_ID in \n",
    "hadm_ids_test list.\n",
    "'''\n",
    "codes_test = torch.zeros((len(hadm_ids_test), 6984), dtype=bool)\n",
    "for index, hadm_id in enumerate(hadm_ids_test):\n",
    "  vec = torch.tensor(codes.loc[hadm_id].to_numpy())\n",
    "  codes_test[index] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1de3f6",
   "metadata": {
    "id": "5e1de3f6"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Load the Doc2Vec embeddings for discharge summary reports, generated during pre-processing step. The data is in \n",
    "Gensim KeyedVector format.\n",
    "'''\n",
    "\n",
    "dv = KeyedVectors.load(PROJECT_DIR + 'data/dv.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yVbin_2K7vEX",
   "metadata": {
    "id": "yVbin_2K7vEX"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create the training tensor consisting of a Doc2Vec embedding in each row, where each row corresponds to HADM_ID in \n",
    "hadm_ids_train list.\n",
    "'''\n",
    "\n",
    "dv_train = torch.zeros((len(hadm_ids_train), 128))\n",
    "for index, hadm_id in enumerate(hadm_ids_train):\n",
    "  vec = torch.Tensor(dv[str(hadm_id)].tolist())\n",
    "  dv_train[index] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KCXKj3vX-2gM",
   "metadata": {
    "id": "KCXKj3vX-2gM"
   },
   "outputs": [],
   "source": [
    "# Push the tensor to GPU memory. Presence of pre-processed data in GPU memory helps improving the performance.\n",
    "\n",
    "dv_train = dv_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wgzvcmmQqWni",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wgzvcmmQqWni",
    "outputId": "05319a6c-e347-4190-f0f2-532211f42c4e"
   },
   "outputs": [],
   "source": [
    "dv_train.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8JgU7sPv-7xa",
   "metadata": {
    "id": "8JgU7sPv-7xa"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create the test tensor consisting of a Doc2Vec embedding in each row, where each row corresponds to HADM_ID in \n",
    "hadm_ids_test list.\n",
    "'''\n",
    "\n",
    "dv_test = torch.zeros((len(hadm_ids_test), 128))\n",
    "for index, hadm_id in enumerate(hadm_ids_test):\n",
    "  vec = torch.Tensor(dv[str(hadm_id)].tolist())\n",
    "  dv_test[index] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IIRhPp3h_K_j",
   "metadata": {
    "id": "IIRhPp3h_K_j"
   },
   "outputs": [],
   "source": [
    "# Push the tensor to GPU memory.\n",
    "\n",
    "dv_test = dv_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71e938",
   "metadata": {
    "id": "2d71e938"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Load the Word2Vec embeddings of all the words in vocabulary of the whole corpus, generated during the \n",
    "pre-processing step. This data is in Gensim KeyedVector format.\n",
    "'''\n",
    "\n",
    "wv = KeyedVectors.load(PROJECT_DIR + 'data/wv.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KyuZNasTQ_Dm",
   "metadata": {
    "id": "KyuZNasTQ_Dm"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Load the dictionary mapping HADM_ID with a tokenized document. These tokens are basically the list of words \n",
    "belonging to the corresponding Discharge summary report.\n",
    "'''\n",
    "\n",
    "with open(PROJECT_DIR + 'data/tokens_map.pkl', 'rb') as handle:\n",
    "  tokens_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_oT0y9axlFzp",
   "metadata": {
    "id": "_oT0y9axlFzp"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create the training tensor consisting of a concatenated Word2Vec embeddings of all the words in the given document, \n",
    "in each row, where each row corresponds to HADM_ID in hadm_ids_train list. This is a very slow running step, \n",
    "but once done, helps train the model faster.\n",
    "'''\n",
    "\n",
    "tokens_train = torch.zeros((len(hadm_ids_train), 70000))\n",
    "for index, hadm_id in enumerate(hadm_ids_train):\n",
    "  tokens = tokens_dict[hadm_id]\n",
    "  word_vecs = torch.Tensor(wv.__getitem__(tokens).flatten().tolist())\n",
    "  tokens_train[index][0:len(word_vecs)] = word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OiFRFw4e7PEb",
   "metadata": {
    "id": "OiFRFw4e7PEb"
   },
   "outputs": [],
   "source": [
    "# Push the tokens_train tensor to GPU memory.\n",
    "\n",
    "tokens_train = tokens_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fu5p4IsXL_y9",
   "metadata": {
    "id": "Fu5p4IsXL_y9"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create the test tensor consisting of a concatenated Word2Vec embeddings of all the words in the given document, in \n",
    "each row, where each row corresponds to HADM_ID in hadm_ids_test list.\n",
    "'''\n",
    "\n",
    "tokens_test = torch.zeros((len(hadm_ids_test), 70000))\n",
    "for index, hadm_id in enumerate(hadm_ids_test):\n",
    "  tokens = tokens_dict[hadm_id]\n",
    "  word_vecs = torch.Tensor(wv.__getitem__(tokens).flatten().tolist())\n",
    "  tokens_test[index][0:len(word_vecs)] = word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owCpvxud7hm_",
   "metadata": {
    "id": "owCpvxud7hm_"
   },
   "outputs": [],
   "source": [
    "# Push the tensor to GPU memory.\n",
    "\n",
    "tokens_test = tokens_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b7f611",
   "metadata": {
    "id": "d8b7f611"
   },
   "outputs": [],
   "source": [
    "# how many samples per batch to load\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a0882",
   "metadata": {
    "id": "710a0882"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Define the implementation of PyTorch Dataset. This is very light-weight, as all the data tensors (dv_train, \n",
    "dv_test, tokens_train, tokens_test, and codes_train) have already been pushed to GPU memory. So they can be \n",
    "referenced by HADM_ID index. This dataset simply returns the input index as the data in __getitem()__ method, which \n",
    "will be used during model training to access the input and output data from data tensors.\n",
    "'''\n",
    "\n",
    "class DocumentsDataset(Dataset):\n",
    "    def __init__(self, count):\n",
    "        super(DocumentsDataset).__init__()\n",
    "        self.count = count\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "    def __getitem__(self, idx):  \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476af18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d476af18",
    "outputId": "cd26ffa9-5452-457e-9c8a-745311e0024d"
   },
   "outputs": [],
   "source": [
    "# prepare dataloaders\n",
    "train_loader = DataLoader(DocumentsDataset(len(hadm_ids_train)), batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(DocumentsDataset(len(hadm_ids_test)), batch_size = batch_size)\n",
    "\n",
    "print(\"# of train batches:\", len(train_loader))\n",
    "print(\"# of val batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d6695",
   "metadata": {},
   "source": [
    "## Create DL Model\n",
    "\n",
    "The model is a DL network with two \"logical\" components:\n",
    "- **Encoder** to generate document embeddings: The function of this component is to generate effective fixed-length embedding for a given discharge summary document.This component consists of two \"logical\" sub-components:\n",
    "        - D2V: This sub-component first trains (as pre-processing step) Doc2Vec model to learn input document vectors of length `128`, in an unsupervised way. It then fine tunes this vector,using a fully connected layer of `64` neurons, followed by a non-linear activation like sigmoid. This fine-tune layer is trained in supervised way.\n",
    "        - CNN: This sub-component trains a Word2Vec model as pre-processing step to build word vectors for the whole vocabulary of the collective corpus of documents. For each document, all the vectors corresponding to the contained words, are concatenated, to represent the given document. These document vectors are used as input to the CNN sub-component. This sub-component actually comprises of 3 single-layer multi-channel CNN models. Three CNN models correspond to 3 kernel sizes (of 3, 4, and 5 words)) with 64 output channels each. For CNN layer in each model is followed by a MaxPool layer to perform temporal pooling. The outputs of each of these CNN models are concatenated to generate the output vector per document of size `192 (3 models * 64 channels each)`. \n",
    "\n",
    "The ouput vectors from the two sub-components (D2V and CNN) are concatenated to produce the final vector for each document in the batch. Ths final vector size is `256 (64 from DNN + 192 from CNN)`.\n",
    "\n",
    "- **Classifier** to perform multi-label classification of ICD-9 codes. This component consists of:\n",
    "    - Dropout layer: The document vector generated by encoder component is regularized by stochastically dropping different dimensions.\n",
    "\t- Fully connected layer with sigmoid activation: This layer generates the final output of size `6984` (total number of ICD-9 codes)}. Each dimension (representing an ICD-9 code) is assigned a probability by sigmoid activation.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./architecture.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d57a5",
   "metadata": {
    "id": "772d57a5"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Define the model class which represents the D2V sub-component that uses single fully connected layer to fine-tune \n",
    "the Doc2Vec embeddings. The output is a vector representing the input document.\n",
    "'''\n",
    "\n",
    "class D2V(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D2V, self).__init__()\n",
    "        self.fc = nn.Linear(128, 64)\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.fc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbede23a",
   "metadata": {
    "id": "dbede23a"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Define the model class which is the building block of the CNN sub-component containining a single layer of \n",
    "multi-channel CNN kernel, activation function, and max-pooling layer.\n",
    "'''\n",
    "# Define the model class which performs 1D-CNN and max-pooling. Details in starting cell.\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.Conv1d(1, 64, kernel_size)\n",
    "        self.pool = nn.MaxPool1d(70000 - kernel_size + 1)\n",
    "    def forward(self, x):\n",
    "        return self.pool(torch.sigmoid(self.conv(x))).squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae0791",
   "metadata": {
    "id": "fdae0791"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Define the model class for CNN sub-component. This component concatenates the output of 3 CNN components (described \n",
    "previously), each corresponding to different kernel size (3, 4, and 5 words). Its input is \"concatenated\" Word2Vec \n",
    "embeddings of all words within a document, which it passes to the three CNN components parallely, and then combines \n",
    "their output to create a vector representing the input document.\n",
    "'''\n",
    "\n",
    "class CNN_COMBINED(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_COMBINED, self).__init__()\n",
    "        self.conv_3 = CNN(300)\n",
    "        self.conv_4 = CNN(400)\n",
    "        self.conv_5 = CNN(500)\n",
    "    def forward(self, x):\n",
    "        return torch.cat((self.conv_3(x), self.conv_4(x), self.conv_5(x)), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88334e11",
   "metadata": {
    "id": "88334e11"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Define the class for the main model, which concatenates the output from D2V and CNN_COMBINED sub-components, as the \n",
    "final vector representation of the input document. It is the input to the multi-label classification task. The \n",
    "classification layer consists of a dropout layer to achieve regularization by stochastically dropping different \n",
    "dimensions of input document vector. It is followed by a fully connected layer with sigmoid activation: This layer \n",
    "generates the final output vector of size of 6984 (total number of ICD-9 codes)}. Each dimension (representing an \n",
    "ICD-9 code) is assigned a probability by sigmoid activation.\n",
    "'''\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.d2v = D2V()\n",
    "        self.cnn = CNN_COMBINED()\n",
    "        self.fc2 = nn.Linear(256, 6984)\n",
    "        self.dropout = nn.Dropout(p = 0.20)\n",
    "\n",
    "    def forward(self, x_indexes, train = True):\n",
    "        x_d2v = dv_train[x_indexes] if train else dv_test[x_indexes]\n",
    "        x_cnn = tokens_train[x_indexes] if train else tokens_test[x_indexes]\n",
    "        x_cnn = x_cnn.unsqueeze(dim = 1)\n",
    "        y_d2v = self.d2v(x_d2v)\n",
    "        y_cnn = self.cnn(x_cnn)\n",
    "        return torch.sigmoid(self.fc2(self.dropout(torch.cat((y_d2v, y_cnn), dim = 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fc66cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6fc66cb",
    "outputId": "aae504e9-94b0-4fe5-d455-037bf9e1de59"
   },
   "outputs": [],
   "source": [
    "# Initialize the model and push it to GPU memory.\n",
    "\n",
    "model = Net()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469743b",
   "metadata": {
    "id": "e469743b"
   },
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer for back-propagation.\n",
    " \n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5882723c",
   "metadata": {
    "id": "5882723c"
   },
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "def classification_metrics(Y_pred, Y_true):\n",
    "    \"\"\"\n",
    "    Calculate peformance metrics using scikit-learn.\n",
    "    \n",
    "    Arguments:\n",
    "        Y_pred: Long dtype Tensor of output values for the test set batch, as predicted by model.\n",
    "        Y_true: Long dtype Tensor of true values in the test-set batch.\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall micro-averaged precision score\n",
    "        recall: overall micro-averaged recall score\n",
    "        f1: overall micro-averaged f1 score\n",
    "        \n",
    "    REFERENCE: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "\"\"\"\n",
    "    \n",
    "    precision, recall, f1score = precision_score(Y_true, Y_pred, average = 'micro'), \\\n",
    "                                           recall_score(Y_true, Y_pred, average = 'micro'), \\\n",
    "                                           f1_score(Y_true, Y_pred, average = 'micro')\n",
    "    return precision, recall, f1score\n",
    "\n",
    "\n",
    "def evaluate(model, loader, threshold):\n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: Trained model of type nn.Module\n",
    "        loader: Test DataLoader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall micro-averaged precision score\n",
    "        recall: overall micro-averaged recall score\n",
    "        f1: overall micro-averaged f1 score\n",
    "\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_y_true = torch.LongTensor()\n",
    "    all_y_pred = torch.LongTensor()\n",
    "    for x_indexes in loader:\n",
    "        x_indexes = x_indexes.long()\n",
    "        y = codes_test[x_indexes]\n",
    "        x_indexes = x_indexes.to(device)\n",
    "        y_hat = model(x_indexes, False)\n",
    "        y_pred = y_hat.detach().to('cpu').apply_(lambda x: 1 if x > threshold else 0)\n",
    "        all_y_true = torch.cat((all_y_true, y.long()), dim=0)\n",
    "        all_y_pred = torch.cat((all_y_pred,  y_pred.to('cpu').long()), dim=0)\n",
    "        \n",
    "    precision, recall, f1 = classification_metrics(all_y_pred.detach().numpy(), all_y_true.detach().numpy())\n",
    "    print(f\"precision: {precision:.3f}, recall: {recall:.3f}, f1: {f1:.3f}\")\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ot4-ISeqoyMo",
   "metadata": {
    "id": "ot4-ISeqoyMo"
   },
   "outputs": [],
   "source": [
    "# Define the epoch counter. This is defined separately to facilitate multi-step training.\n",
    "epoch = 0\n",
    "#train_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d7e8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c71d7e8e",
    "outputId": "6b9360a8-24d6-41fb-f2bc-2ef8dad2e5ec"
   },
   "outputs": [],
   "source": [
    "# Model Training\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 80\n",
    "\n",
    "# The classification probability thershold. \n",
    "threshold = 0.20\n",
    "\n",
    "sta = time.time()\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    # prep model for training\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for x_indexes in train_loader:\n",
    "        x_indexes = x_indexes.long()\n",
    "        x_indexes = x_indexes.to(device)\n",
    "        y = codes_train[x_indexes].float()\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x_indexes, True)\n",
    "        loss = criterion(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch: {} done!'.format(epoch))\n",
    "    if epoch % 2 == 0:\n",
    "        evaluate(model, test_loader, threshold)\n",
    "    epoch += 1\n",
    "\n",
    "end = time.time()\n",
    "print('Time spent:' + str(end - sta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mH7ZsUOgESSQ",
   "metadata": {
    "id": "mH7ZsUOgESSQ"
   },
   "outputs": [],
   "source": [
    "# Save model internal state as intermediate checkpoint to facilitate multi-step training.\n",
    "\n",
    "torch.save(model.state_dict(), PROJECT_DIR + 'data/checkpoint.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
