{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkC1RqNMYvwd"
   },
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- We need to import and transform following MIMIC-III datasets:\n",
    "  \n",
    "  DIAGNOSES_ICD.csv: Each row in this file maps Hospitalization ID (HADM_ID) of a patient with a unique ICD_9 CODE.\n",
    "  \n",
    "  Example:\n",
    "  \n",
    "  |ROW_ID|SUBJECT_ID|**HADM_ID**|SEQ_NUM|**ICD9_CODE**|\n",
    "  |--------|------------|---------|---------|-----------|\n",
    "  |1297|109|172335|1|\"0030\"|\n",
    "  |1299|109|172335|3|\"0038\"|\n",
    "  |1301|109|173633|5|\"0031\"|\n",
    "  \n",
    "  \n",
    "  The aim is to transform this dataset to dataframe DIGNOSES with index as HADM_ID and columns as unique ICD_9 codes (6984 in total), to represent multi-hot encoding of ICD_9 codes for given hospitalization.\n",
    "  \n",
    "  |HADM_ID|ICD9_CODE_0030|ICD9_CODE_0031|ICD9_CODE_0038|\n",
    "  |-------|--------------|--------------|--------------|\n",
    "  |172335|1|0|1|\n",
    "  |173633|0|1|0|\n",
    "  \n",
    "  \n",
    "  \n",
    "  NOTEEVENTS.csv: Each row maps HADM_ID (Hospitalization ID) with a free text Discharge summary (TEXT) field.\n",
    "  \n",
    "  |ROW_ID|SUBJECT_ID|**HADM_ID**|CHARTDATE|CHARTTIME|STORETIME|CATEGORY|DESCRIPTION|CGID|ISERROR|**TEXT**|\n",
    "  |------|----------|-----------|---------|---------|---------|--------|-----------|----|-------|--------|\n",
    "  |174|22532|167853|2151-08-04|||Discharge summary|Report|||Admission Date:  [\\*\\*2151-7-16**]       Discharge Date:  [\\*\\*2151-8-4**] Service: ADDENDUM: RADIOLOGIC STUDIES:  Radiologic studies also included a chest| \n",
    " \n",
    "  The aim is to transform this to dataframe with HADM_ID as index and TEXT as column.\n",
    "  |HADM_ID|TEXT|\n",
    "  |-------|----|\n",
    "  |167853|Admission Date:  [\\*\\*2151-7-16**]       Discharge Date:  [\\*\\*2151-8-4**] Service: ADDENDUM: RADIOLOGIC STUDIES:  Radiologic studies also included a chest|\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "- The processed dataframes are then stored (as csv) for further usage.\n",
    "\n",
    "- This notebook uses Rapids framework (cudf and dask dataframe) to enable faster processing of Pandas dataframe on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCr_TZjqZDve"
   },
   "source": [
    "#### Check GPU Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nAj-Ie3nv-aM",
    "outputId": "8b44f464-b0fd-4c06-b1e9-ac9f30919c4a"
   },
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OyjkVky6viJ"
   },
   "source": [
    "#Setup:\n",
    "This set up script:\n",
    "\n",
    "1. Checks to make sure that the GPU is RAPIDS compatible\n",
    "1. Installs the **current stable version** of RAPIDSAI's core libraries using pip, which are:\n",
    "  1. cuDF\n",
    "  1. cuML\n",
    "  1. cuGraph\n",
    "  1. xgboost\n",
    "\n",
    "**This will complete in about 3-4 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94Fdr-lt6wGv",
    "outputId": "c4decf6d-7e09-4f6a-b2dd-45cb000f8ff4"
   },
   "outputs": [],
   "source": [
    "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
    "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
    "\n",
    "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "!python rapidsai-csp-utils/colab/pip-install.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fweFyNRw00So"
   },
   "source": [
    "## Critical Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hUagBPMOuMAy",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b3b0c0c0-6332-4887-886c-0edee300680a"
   },
   "outputs": [],
   "source": [
    "# Critical imports\n",
    "import cudf\n",
    "import cuml\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount the project directory in Google drive. (Its only intended to be run in colab environment.)\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base project directory.\n",
    "\n",
    "PROJECT_DIR = 'drive/My Drive/cs598-dl/' # For Google drive only\n",
    "\n",
    "# PROJECT_DIR = '../' # For local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first process MIMIC-III DIAGNOSES_ICD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xs4q87VD8GPA",
    "outputId": "0130aded-3d75-4ac6-a8d0-92c9d10abceb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read DIAGNOSES_ICD.csv from data directory, and pre-process.\n",
    "\n",
    "diagnoses_df = pd.read_csv(PROJECT_DIR + 'data/DIAGNOSES_ICD.csv', usecols=['HADM_ID', 'ICD9_CODE'])\n",
    "diagnoses_df = diagnoses_df.astype({'ICD9_CODE': 'string'})\n",
    "\n",
    "# Collect all unique ICD_9 codes and create new DataFrame codes_df.\n",
    "codes_df = pd.DataFrame(diagnoses_df['ICD9_CODE'].unique(), columns = ['ICD9_CODE'])\n",
    "\n",
    "# Create DataFrame representing one-hot encoding of ICD_9 codes.\n",
    "one_hot_enc_df = pd.get_dummies(codes_df, columns = ['ICD9_CODE'], dtype='bool')\n",
    "\n",
    "# Join codes_df and one_hot_enc_df, based on index\n",
    "codes_df = codes_df.join(one_hot_enc_df)\n",
    "\n",
    "# Next we merge diagnoses_df and codes_df, to create our final form mapping each HADM_ID with multi-hot encoding of ICD_9 codes.\n",
    "# This is very heavy operation due to large number of rows in diagnoses_df and large number of columns in codes_df.\n",
    "# So we utilize Dask DataFrame to parallelize this operation on GPU cores.\n",
    "\n",
    "# Create Dask DataFrame from codes_df for distributed processing in GPU.\n",
    "codes_df = dd.from_pandas(codes_df, npartitions = 10)\n",
    "\n",
    "# Create Dask DataFrame from diagnoses_df for distributed processing in GPU.\n",
    "diagnoses_df = dd.from_pandas(diagnoses_df, npartitions = 10)\n",
    "\n",
    "# Merge diagnoses_df and codes_df based on column 'ICD9_CODE'. \n",
    "# Dask operations are lazy and do not materialize until 'compute()' method is invoked.\n",
    "diagnoses_df = diagnoses_df.merge(codes_df, on='ICD9_CODE').compute()\n",
    "\n",
    "diagnoses_df = diagnoses_df.drop(['ICD9_CODE'], axis = 1)\n",
    "\n",
    "# This step will group all ICD_9 codes corresponding to a given HADM_ID and build a multi-hot embedding.\n",
    "diagnoses_df = diagnoses_df.groupby('HADM_ID').any().reset_index()\n",
    "\n",
    "print(diagnoses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Read DIAGNOSES_ICD.csv from data directory, and pre-process.\n",
    "\n",
    "diagnoses_df = pd.read_csv(PROJECT_DIR + 'data/DIAGNOSES_ICD.csv', usecols=['HADM_ID', 'ICD9_CODE'])\n",
    "diagnoses_df = diagnoses_df.astype({'ICD9_CODE': 'string'})\n",
    "\n",
    "# Collect all unique ICD_9 codes and create new DataFrame codes_df.\n",
    "codes_df = pd.DataFrame(diagnoses_df['ICD9_CODE'].unique(), columns = ['ICD9_CODE'])\n",
    "\n",
    "# Create DataFrame representing one-hot encoding of ICD_9 codes.\n",
    "one_hot_enc_df = pd.get_dummies(codes_df, columns = ['ICD9_CODE'], dtype='bool')\n",
    "\n",
    "# Join codes_df and one_hot_enc_df, based on index\n",
    "codes_df = codes_df.join(one_hot_enc_df)\n",
    "\n",
    "sta = time.time()\n",
    "# Merge diagnoses_df and codes_df based on column 'ICD9_CODE'. \n",
    "# Dask operations are lazy and do not materialize until 'compute()' method is invoked.\n",
    "diagnoses_df = diagnoses_df.merge(codes_df, on='ICD9_CODE')\n",
    "end = time.time()\n",
    "print(end - sta)\n",
    "\n",
    "diagnoses_df = diagnoses_df.drop(['ICD9_CODE'], axis = 1)\n",
    "\n",
    "# This step will group all ICD_9 codes corresponding to a given HADM_ID and build a multi-hot embedding.\n",
    "diagnoses_df = diagnoses_df.groupby('HADM_ID').any().reset_index()\n",
    "\n",
    "print(diagnoses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we process MIMIC-III NOTEEVENTS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHS81ejCQulU",
    "outputId": "33e7eb69-4823-4884-93a2-45491bb65d57"
   },
   "outputs": [],
   "source": [
    "# Import dataset and pre-process.\n",
    "notes_df = pd.read_csv('drive/My Drive/cs598-dl/data/NOTEEVENTS.csv', usecols=['HADM_ID', \"CATEGORY\",\"DESCRIPTION\", \"TEXT\"])\n",
    "notes_df = notes_df.dropna()\n",
    "\n",
    "# Only filter-in notes which are 'Discharge summary' and are of sub-type 'Report'.\n",
    "notes_df = notes_df[(notes_df['CATEGORY'] == 'Discharge summary') & (notes_df['DESCRIPTION'] == 'Report')]\n",
    "notes_df = notes_df.drop(['CATEGORY', 'DESCRIPTION'], axis=1)\n",
    "notes_df = notes_df.astype({'HADM_ID': 'int64'})\n",
    "notes_df = notes_df.drop_duplicates(subset = 'HADM_ID')\n",
    "print(notes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We next select the subset of rows in diagnoses_df and notes_df with common set of HADM_IDs, \n",
    "# and remove other rows from each DataFrame. Such rows can not be used in training or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HcpXLsjZad0K",
    "outputId": "1da96b16-790e-4e08-9e13-0a68424bb6b5"
   },
   "outputs": [],
   "source": [
    "# Collect all hadm_ids from diagnoses_df\n",
    "hadm_ids_from_diagnoses_df = diagnoses_df.filter(items = ['HADM_ID'])\n",
    "\n",
    "# Collect all hadm_ids from diagnoses_df\n",
    "hadm_ids_from_notes_df = notes_df.filter(items = ['HADM_ID'])\n",
    "\n",
    "# Generate DataFrame with common set of HADM_IDs.\n",
    "hadm_ids_df = hadm_ids_from_diagnoses_df.merge(hadm_ids_from_notes_df, how = 'inner')\n",
    "\n",
    "# Filter rows in daignoses_df by merging with DataFrame containing common HADM_IDs.\n",
    "diagnoses_df = diagnoses_df.merge(hadm_ids_df, on='HADM_ID', how = 'right')\n",
    "\n",
    "# Similarly, filter rows in notes_df by merging with DataFrame containing common HADM_IDs.\n",
    "notes_df = notes_df.merge(hadm_ids_df, on='HADM_ID', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUKIuqsLvCvz"
   },
   "outputs": [],
   "source": [
    "# Pickle diagnoses_df\n",
    "diagnoses_df.to_pickle(PROJECT_DIR + 'data/DIAGNOSES.pkl')\n",
    "\n",
    "# Pickle notes_df\n",
    "notes_df.to_pickle(PROJECT_DIR + 'data/NOTES.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_df = Da"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-rapids] *",
   "language": "python",
   "name": "conda-env-.conda-rapids-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
