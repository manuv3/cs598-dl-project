{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkC1RqNMYvwd"
   },
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- We need to import and transform following MIMIC-III datasets:\n",
    "  \n",
    "  DIAGNOSES_ICD.csv: Each row in this file maps Hospitalization ID (HADM_ID) of a patient with a unique ICD_9 CODE.\n",
    "  \n",
    "  Example:\n",
    "  \n",
    "  |ROW_ID|SUBJECT_ID|**HADM_ID**|SEQ_NUM|**ICD9_CODE**|\n",
    "  |--------|------------|---------|---------|-----------|\n",
    "  |1297|109|172335|1|\"0030\"|\n",
    "  |1299|109|172335|3|\"0038\"|\n",
    "  |1301|109|173633|5|\"0031\"|\n",
    "  \n",
    "  \n",
    "  The aim is to transform this dataset to dataframe DIGNOSES with index as HADM_ID and columns as unique ICD_9 codes (6984 in total), to represent multi-hot encoding of ICD_9 codes for given hospitalization.\n",
    "  \n",
    "  |HADM_ID|ICD9_CODE_0030|ICD9_CODE_0031|ICD9_CODE_0038|\n",
    "  |-------|--------------|--------------|--------------|\n",
    "  |172335|1|0|1|\n",
    "  |173633|0|1|0|\n",
    "  \n",
    "  \n",
    "  \n",
    "  NOTEEVENTS.csv: Each row maps HADM_ID (Hospitalization ID) with a free text Discharge summary (TEXT) field.\n",
    "  \n",
    "  |ROW_ID|SUBJECT_ID|**HADM_ID**|CHARTDATE|CHARTTIME|STORETIME|CATEGORY|DESCRIPTION|CGID|ISERROR|**TEXT**|\n",
    "  |------|----------|-----------|---------|---------|---------|--------|-----------|----|-------|--------|\n",
    "  |174|22532|167853|2151-08-04|||Discharge summary|Report|||Admission Date:  [\\*\\*2151-7-16**]       Discharge Date:  [\\*\\*2151-8-4**] Service: ADDENDUM: RADIOLOGIC STUDIES:  Radiologic studies also included a chest| \n",
    " \n",
    "  The aim is to transform this to dataframe with HADM_ID as index and TEXT as column.\n",
    "  |HADM_ID|TEXT|\n",
    "  |-------|----|\n",
    "  |167853|Admission Date:  [\\*\\*2151-7-16**]       Discharge Date:  [\\*\\*2151-8-4**] Service: ADDENDUM: RADIOLOGIC STUDIES:  Radiologic studies also included a chest|\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "- The processed dataframes are then stored (as csv) for further usage.\n",
    "\n",
    "- This notebook uses Rapids framework (cudf and dask dataframe) to enable faster processing of Pandas dataframe on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCr_TZjqZDve"
   },
   "source": [
    "#### Check GPU Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nAj-Ie3nv-aM",
    "outputId": "8b44f464-b0fd-4c06-b1e9-ac9f30919c4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 15 19:43:17 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.78.01    Driver Version: 525.78.01    CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   45C    P8     2W /  50W |      3MiB /  4096MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      2328      G   /usr/bin/gnome-shell                1MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OyjkVky6viJ"
   },
   "source": [
    "#Setup:\n",
    "This set up script:\n",
    "\n",
    "1. Checks to make sure that the GPU is RAPIDS compatible\n",
    "1. Installs the **current stable version** of RAPIDSAI's core libraries using pip, which are:\n",
    "  1. cuDF\n",
    "  1. cuML\n",
    "  1. cuGraph\n",
    "  1. xgboost\n",
    "\n",
    "**This will complete in about 3-4 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94Fdr-lt6wGv",
    "outputId": "c4decf6d-7e09-4f6a-b2dd-45cb000f8ff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'rapidsai-csp-utils'...\n",
      "remote: Enumerating objects: 385, done.\u001b[K\n",
      "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
      "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
      "remote: Total 385 (delta 86), reused 51 (delta 51), pack-reused 269\u001b[K\n",
      "Receiving objects: 100% (385/385), 105.74 KiB | 2.20 MiB/s, done.\n",
      "Resolving deltas: 100% (188/188), done.\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pynvml\n",
      "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 KB 2.7 MB/s eta 0:00:00\n",
      "Installing collected packages: pynvml\n",
      "Successfully installed pynvml-11.5.0\n",
      "***********************************************************************\n",
      "Woo! Your instance has the right kind of GPU, a Tesla T4!\n",
      "We will now install RAPIDS cuDF, cuML, and cuGraph via pip! \n",
      "Please stand by, should be quick...\n",
      "***********************************************************************\n",
      "\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.nvidia.com\n",
      "Collecting cudf-cu11\n",
      "  Downloading https://pypi.nvidia.com/cudf-cu11/cudf_cu11-23.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (491.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 491.0/491.0 MB 3.3 MB/s eta 0:00:00\n",
      "Collecting cuml-cu11\n",
      "  Downloading https://pypi.nvidia.com/cuml-cu11/cuml_cu11-23.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1586.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 GB 1.1 MB/s eta 0:00:00\n",
      "Collecting cugraph-cu11\n",
      "  Downloading https://pypi.nvidia.com/cugraph-cu11/cugraph_cu11-23.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1246.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 GB 1.4 MB/s eta 0:00:00\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 17.4 MB/s eta 0:00:00\n",
      "Collecting cuda-python<12.0,>=11.7.1\n",
      "  Downloading cuda_python-11.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.5/16.5 MB 86.9 MB/s eta 0:00:00\n",
      "Collecting pyarrow==10\n",
      "  Downloading pyarrow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35.2/35.2 MB 19.7 MB/s eta 0:00:00\n",
      "Collecting cubinlinker-cu11\n",
      "  Downloading https://pypi.nvidia.com/cubinlinker-cu11/cubinlinker_cu11-0.3.0.post1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 71.8 MB/s eta 0:00:00\n",
      "Collecting protobuf==4.21\n",
      "  Downloading protobuf-4.21.0-cp37-abi3-manylinux2014_x86_64.whl (407 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 407.6/407.6 KB 36.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from cudf-cu11) (1.22.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from cudf-cu11) (4.5.0)\n",
      "Requirement already satisfied: pandas<1.6.0dev0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from cudf-cu11) (1.4.4)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from cudf-cu11) (5.3.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from cudf-cu11) (2023.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from cudf-cu11) (23.0)\n",
      "Requirement already satisfied: cupy-cuda11x in /usr/local/lib/python3.9/dist-packages (from cudf-cu11) (11.0.0)\n",
      "Collecting nvtx>=0.2.1\n",
      "  Downloading nvtx-0.2.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 441.3/441.3 KB 46.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numba>=0.56.2 in /usr/local/lib/python3.9/dist-packages (from cudf-cu11) (0.56.4)\n",
      "Collecting rmm-cu11==23.2.*\n",
      "  Downloading https://pypi.nvidia.com/rmm-cu11/rmm_cu11-23.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 90.0 MB/s eta 0:00:00\n",
      "Collecting ptxcompiler-cu11\n",
      "  Downloading https://pypi.nvidia.com/ptxcompiler-cu11/ptxcompiler_cu11-0.7.0.post1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 84.4 MB/s eta 0:00:00\n",
      "Collecting treelite==3.1.0\n",
      "  Downloading treelite-3.1.0-py3-none-manylinux2014_x86_64.whl (873 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 873.5/873.5 KB 47.3 MB/s eta 0:00:00\n",
      "Collecting raft-dask-cu11==23.2.*\n",
      "  Downloading https://pypi.nvidia.com/raft-dask-cu11/raft_dask_cu11-23.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (215.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 215.1/215.1 MB 6.6 MB/s eta 0:00:00\n",
      "Collecting dask-cudf-cu11==23.2.*\n",
      "  Downloading https://pypi.nvidia.com/dask-cudf-cu11/dask_cudf_cu11-23.2.0-py3-none-any.whl (76 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.1/76.1 KB 11.0 MB/s eta 0:00:00\n",
      "Collecting pylibraft-cu11==23.2.*\n",
      "  Downloading https://pypi.nvidia.com/pylibraft-cu11/pylibraft_cu11-23.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1015.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 GB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from cuml-cu11) (1.10.1)\n",
      "Collecting treelite-runtime==3.1.0\n",
      "  Downloading treelite_runtime-3.1.0-py3-none-manylinux2014_x86_64.whl (191 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 191.9/191.9 KB 23.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.9/dist-packages (from cuml-cu11) (0.12.2)\n",
      "Collecting distributed==2023.1.1\n",
      "  Downloading distributed-2023.1.1-py3-none-any.whl (934 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 934.1/934.1 KB 66.1 MB/s eta 0:00:00\n",
      "Collecting dask==2023.1.1\n",
      "  Downloading dask-2023.1.1-py3-none-any.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 67.7 MB/s eta 0:00:00\n",
      "Collecting ucx-py-cu11==0.30.*\n",
      "  Downloading https://pypi.nvidia.com/ucx-py-cu11/ucx_py_cu11-0.30.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.4/8.4 MB 48.7 MB/s eta 0:00:00\n",
      "Collecting dask-cuda==23.2.*\n",
      "  Downloading dask_cuda-23.2.1-py3-none-any.whl (240 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.2/240.2 KB 23.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/dist-packages (from raft-dask-cu11==23.2.*->cuml-cu11) (1.1.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from dask==2023.1.1->dask-cudf-cu11==23.2.*->cuml-cu11) (8.1.3)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.9/dist-packages (from dask==2023.1.1->dask-cudf-cu11==23.2.*->cuml-cu11) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from dask==2023.1.1->dask-cudf-cu11==23.2.*->cuml-cu11) (6.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from dask==2023.1.1->dask-cudf-cu11==23.2.*->cuml-cu11) (2.2.1)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.9/dist-packages (from dask==2023.1.1->dask-cudf-cu11==23.2.*->cuml-cu11) (1.3.0)\n",
      "Collecting pynvml<11.5,>=11.0.0\n",
      "  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.0/47.0 KB 5.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.9/dist-packages (from dask-cuda==23.2.*->raft-dask-cu11==23.2.*->cuml-cu11) (2.2.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.9/dist-packages (from distributed==2023.1.1->dask-cudf-cu11==23.2.*->cuml-cu11) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from distributed==2023.1.1->dask-cudf-cu11==23.2.*->cuml-cu11) (1.7.0)\n",
      "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from distributed==2023.1.1->dask-cudf-cu11==23.2.*->cuml-cu11) (1.0.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.9/dist-packages (from distributed==2023.1.1->dask-cudf-cu11==23.2.*->cuml-cu11) (6.2)\n",
      "Requirement already satisfied: psutil>=5.7.0 in /usr/local/lib/python3.9/dist-packages (from distributed==2023.1.1->dask-cudf-cu11==23.2.*->cuml-cu11) (5.9.4)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from distributed==2023.1.1->dask-cudf-cu11==23.2.*->cuml-cu11) (1.0.5)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from distributed==2023.1.1->dask-cudf-cu11==23.2.*->cuml-cu11) (1.26.15)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.9/dist-packages (from distributed==2023.1.1->dask-cudf-cu11==23.2.*->cuml-cu11) (3.1.2)\n",
      "Collecting pylibcugraph-cu11==23.2.*\n",
      "  Downloading https://pypi.nvidia.com/pylibcugraph-cu11/pylibcugraph_cu11-23.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1242.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 GB 1.4 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 KB 14.3 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.8/158.8 KB 20.0 MB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 264.6/264.6 KB 22.9 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp) (2.0.12)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp) (22.2.0)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from cuda-python<12.0,>=11.7.1->cudf-cu11) (0.29.34)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.56.2->cudf-cu11) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.56.2->cudf-cu11) (67.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas<1.6.0dev0,>=1.0->cudf-cu11) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas<1.6.0dev0,>=1.0->cudf-cu11) (2022.7.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.9/dist-packages (from yarl<2.0,>=1.0->aiohttp) (3.4)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.9/dist-packages (from cupy-cuda11x->cudf-cu11) (0.8.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.9/dist-packages (from seaborn->cuml-cu11) (3.7.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->cuml-cu11) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->cuml-cu11) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->cuml-cu11) (4.39.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->cuml-cu11) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->cuml-cu11) (5.12.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->cuml-cu11) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->cuml-cu11) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas<1.6.0dev0,>=1.0->cudf-cu11) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.1->seaborn->cuml-cu11) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2>=2.10.3->distributed==2023.1.1->dask-cudf-cu11==23.2.*->cuml-cu11) (2.1.2)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.9/dist-packages (from zict>=0.1.3->dask-cuda==23.2.*->raft-dask-cu11==23.2.*->cuml-cu11) (1.0.1)\n",
      "WARNING: The candidate selected for download or install is a yanked version: 'protobuf' candidate (version 4.21.0 at https://files.pythonhosted.org/packages/9d/82/b3131637daf2a27eab76b0de8e139ecf0f6624832c03531dce8a7d59ddc1/protobuf-4.21.0-cp37-abi3-manylinux2014_x86_64.whl#sha256=9a65e012bc06022e98a57165ea48438b3b9f652eee33db7cbecb883588f9f169 (from https://pypi.org/simple/protobuf/))\n",
      "Reason for being yanked: Required python version not configured correctly (https://github.com/protocolbuffers/protobuf/issues/10076)\n",
      "Installing collected packages: ptxcompiler-cu11, protobuf, nvtx, cubinlinker-cu11, pynvml, pyarrow, multidict, frozenlist, cuda-python, async-timeout, yarl, ucx-py-cu11, treelite-runtime, treelite, rmm-cu11, dask, aiosignal, pylibraft-cu11, distributed, cudf-cu11, aiohttp, pylibcugraph-cu11, dask-cudf-cu11, dask-cuda, raft-dask-cu11, cuml-cu11, cugraph-cu11\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: pynvml\n",
      "    Found existing installation: pynvml 11.5.0\n",
      "    Uninstalling pynvml-11.5.0:\n",
      "      Successfully uninstalled pynvml-11.5.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 9.0.0\n",
      "    Uninstalling pyarrow-9.0.0:\n",
      "      Successfully uninstalled pyarrow-9.0.0\n",
      "  Attempting uninstall: dask\n",
      "    Found existing installation: dask 2022.12.1\n",
      "    Uninstalling dask-2022.12.1:\n",
      "      Successfully uninstalled dask-2022.12.1\n",
      "  Attempting uninstall: distributed\n",
      "    Found existing installation: distributed 2022.12.1\n",
      "    Uninstalling distributed-2022.12.1:\n",
      "      Successfully uninstalled distributed-2022.12.1\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 4.21.0 which is incompatible.\n",
      "tensorflow-metadata 1.13.0 requires protobuf<4,>=3.13, but you have protobuf 4.21.0 which is incompatible.\n",
      "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 10.0.0 which is incompatible.\n",
      "google-cloud-translate 3.8.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 4.21.0 which is incompatible.\n",
      "google-cloud-language 2.6.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 4.21.0 which is incompatible.\n",
      "google-cloud-firestore 2.7.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 4.21.0 which is incompatible.\n",
      "google-cloud-datastore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 4.21.0 which is incompatible.\n",
      "google-cloud-bigquery 3.4.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 4.21.0 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.19.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 4.21.0 which is incompatible.\n",
      "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 4.21.0 which is incompatible.\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 cubinlinker-cu11-0.3.0.post1 cuda-python-11.8.1 cudf-cu11-23.2.0 cugraph-cu11-23.2.0 cuml-cu11-23.2.0 dask-2023.1.1 dask-cuda-23.2.1 dask-cudf-cu11-23.2.0 distributed-2023.1.1 frozenlist-1.3.3 multidict-6.0.4 nvtx-0.2.5 protobuf-4.21.0 ptxcompiler-cu11-0.7.0.post1 pyarrow-10.0.0 pylibcugraph-cu11-23.2.0 pylibraft-cu11-23.2.0 pynvml-11.4.1 raft-dask-cu11-23.2.0 rmm-cu11-23.2.0 treelite-3.1.0 treelite-runtime-3.1.0 ucx-py-cu11-0.30.0 yarl-1.8.2\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: cupy-cuda11x in /usr/local/lib/python3.9/dist-packages (11.0.0)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.9/dist-packages (from cupy-cuda11x) (0.8.1)\n",
      "Requirement already satisfied: numpy<1.26,>=1.20 in /usr/local/lib/python3.9/dist-packages (from cupy-cuda11x) (1.22.4)\n",
      "\n",
      "          ***********************************************************************\n",
      "          The pip install of RAPIDS is complete.\n",
      "          \n",
      "          Please do not run any further installation from the conda based installation methods, as they may cause issues!  \n",
      "          \n",
      "          Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts. \n",
      "r          \n",
      "          Troubleshooting:\n",
      "             - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files. \n",
      "             - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n",
      "          ***********************************************************************\n",
      "          \n"
     ]
    }
   ],
   "source": [
    "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
    "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
    "\n",
    "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "!python rapidsai-csp-utils/colab/pip-install.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fweFyNRw00So"
   },
   "source": [
    "## Critical Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hUagBPMOuMAy",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b3b0c0c0-6332-4887-886c-0edee300680a"
   },
   "outputs": [],
   "source": [
    "# Critical imports\n",
    "import cudf\n",
    "import cuml\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount the project directory in Google drive. (Its only intended to be run in colab environment.)\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base project directory.\n",
    "\n",
    "PROJECT_DIR = 'drive/My Drive/cs598-dl/' # For Google drive only\n",
    "\n",
    "# PROJECT_DIR = '../' # For local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first process MIMIC-III DIAGNOSES_ICD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xs4q87VD8GPA",
    "outputId": "0130aded-3d75-4ac6-a8d0-92c9d10abceb",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'time' has no attribute 'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Join codes_df and one_hot_enc_df, based on index\u001b[39;00m\n\u001b[1;32m     13\u001b[0m codes_df \u001b[38;5;241m=\u001b[39m codes_df\u001b[38;5;241m.\u001b[39mjoin(one_hot_enc_df)\n\u001b[0;32m---> 15\u001b[0m sta \u001b[38;5;241m=\u001b[39m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Merge diagnoses_df and codes_df based on column 'ICD9_CODE'. \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Dask operations are lazy and do not materialize until 'compute()' method is invoked.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m diagnoses_df \u001b[38;5;241m=\u001b[39m diagnoses_df\u001b[38;5;241m.\u001b[39mmerge(codes_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mICD9_CODE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'time' has no attribute 'start'"
     ]
    }
   ],
   "source": [
    "# Read DIAGNOSES_ICD.csv from data directory, and pre-process.\n",
    "\n",
    "diagnoses_df = pd.read_csv(PROJECT_DIR + 'data/DIAGNOSES_ICD.csv', usecols=['HADM_ID', 'ICD9_CODE'])\n",
    "diagnoses_df = diagnoses_df.astype({'ICD9_CODE': 'string'})\n",
    "\n",
    "# Collect all unique ICD_9 codes and create new DataFrame codes_df.\n",
    "codes_df = pd.DataFrame(diagnoses_df['ICD9_CODE'].unique(), columns = ['ICD9_CODE'])\n",
    "\n",
    "# Create DataFrame representing one-hot encoding of ICD_9 codes.\n",
    "one_hot_enc_df = pd.get_dummies(codes_df, columns = ['ICD9_CODE'], dtype='bool')\n",
    "\n",
    "# Join codes_df and one_hot_enc_df, based on index\n",
    "codes_df = codes_df.join(one_hot_enc_df)\n",
    "\n",
    "# Next we merge diagnoses_df and codes_df, to create our final form mapping each HADM_ID with multi-hot encoding of ICD_9 codes.\n",
    "# This is very heavy operation due to large number of rows in diagnoses_df and large number of columns in codes_df.\n",
    "# So we utilize Dask DataFrame to parallelize this operation on GPU cores.\n",
    "\n",
    "# Create Dask DataFrame from codes_df for distributed processing in GPU.\n",
    "codes_df = dd.from_pandas(codes_df, npartitions = 10)\n",
    "\n",
    "# Create Dask DataFrame from diagnoses_df for distributed processing in GPU.\n",
    "diagnoses_df = dd.from_pandas(diagnoses_df, npartitions = 10)\n",
    "\n",
    "# Merge diagnoses_df and codes_df based on column 'ICD9_CODE'. \n",
    "# Dask operations are lazy and do not materialize until 'compute()' method is invoked.\n",
    "diagnoses_df = diagnoses_df.merge(codes_df, on='ICD9_CODE').compute()\n",
    "\n",
    "diagnoses_df = diagnoses_df.drop(['ICD9_CODE'], axis = 1)\n",
    "\n",
    "# This step will group all ICD_9 codes corresponding to a given HADM_ID and build a multi-hot embedding.\n",
    "diagnoses_df = diagnoses_df.groupby('HADM_ID').any().reset_index()\n",
    "\n",
    "print(diagnoses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.143898010253906\n",
      "       HADM_ID  ICD9_CODE_0030  ICD9_CODE_0031  ICD9_CODE_0038  \\\n",
      "0       100001           False           False           False   \n",
      "1       100003           False           False           False   \n",
      "2       100006           False           False           False   \n",
      "3       100007           False           False           False   \n",
      "4       100009           False           False           False   \n",
      "...        ...             ...             ...             ...   \n",
      "58971   199993           False           False           False   \n",
      "58972   199994           False           False           False   \n",
      "58973   199995           False           False           False   \n",
      "58974   199998           False           False           False   \n",
      "58975   199999           False           False           False   \n",
      "\n",
      "       ICD9_CODE_0039  ICD9_CODE_0041  ICD9_CODE_0048  ICD9_CODE_0049  \\\n",
      "0               False           False           False           False   \n",
      "1               False           False           False           False   \n",
      "2               False           False           False           False   \n",
      "3               False           False           False           False   \n",
      "4               False           False           False           False   \n",
      "...               ...             ...             ...             ...   \n",
      "58971           False           False           False           False   \n",
      "58972           False           False           False           False   \n",
      "58973           False           False           False           False   \n",
      "58974           False           False           False           False   \n",
      "58975           False           False           False           False   \n",
      "\n",
      "       ICD9_CODE_0051  ICD9_CODE_00581  ...  ICD9_CODE_V8801  ICD9_CODE_V8811  \\\n",
      "0               False            False  ...            False            False   \n",
      "1               False            False  ...            False            False   \n",
      "2               False            False  ...            False            False   \n",
      "3               False            False  ...            False            False   \n",
      "4               False            False  ...            False            False   \n",
      "...               ...              ...  ...              ...              ...   \n",
      "58971           False            False  ...            False            False   \n",
      "58972           False            False  ...            False            False   \n",
      "58973           False            False  ...            False            False   \n",
      "58974           False            False  ...            False            False   \n",
      "58975           False            False  ...            False            False   \n",
      "\n",
      "       ICD9_CODE_V8812  ICD9_CODE_V8821  ICD9_CODE_V9010  ICD9_CODE_V902  \\\n",
      "0                False            False            False           False   \n",
      "1                False            False            False           False   \n",
      "2                False            False            False           False   \n",
      "3                False            False            False           False   \n",
      "4                False            False            False           False   \n",
      "...                ...              ...              ...             ...   \n",
      "58971            False            False            False           False   \n",
      "58972            False            False            False           False   \n",
      "58973            False            False            False           False   \n",
      "58974            False            False            False           False   \n",
      "58975            False            False            False           False   \n",
      "\n",
      "       ICD9_CODE_V9039  ICD9_CODE_V9081  ICD9_CODE_V9089  ICD9_CODE_V9103  \n",
      "0                False            False            False            False  \n",
      "1                False            False            False            False  \n",
      "2                False            False            False            False  \n",
      "3                False            False            False            False  \n",
      "4                False            False            False            False  \n",
      "...                ...              ...              ...              ...  \n",
      "58971            False            False            False            False  \n",
      "58972            False            False            False            False  \n",
      "58973            False            False            False            False  \n",
      "58974            False            False            False            False  \n",
      "58975            False            False            False            False  \n",
      "\n",
      "[58976 rows x 6985 columns]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Read DIAGNOSES_ICD.csv from data directory, and pre-process.\n",
    "\n",
    "diagnoses_df = pd.read_csv(PROJECT_DIR + 'data/DIAGNOSES_ICD.csv', usecols=['HADM_ID', 'ICD9_CODE'])\n",
    "diagnoses_df = diagnoses_df.astype({'ICD9_CODE': 'string'})\n",
    "\n",
    "# Collect all unique ICD_9 codes and create new DataFrame codes_df.\n",
    "codes_df = pd.DataFrame(diagnoses_df['ICD9_CODE'].unique(), columns = ['ICD9_CODE'])\n",
    "\n",
    "# Create DataFrame representing one-hot encoding of ICD_9 codes.\n",
    "one_hot_enc_df = pd.get_dummies(codes_df, columns = ['ICD9_CODE'], dtype='bool')\n",
    "\n",
    "# Join codes_df and one_hot_enc_df, based on index\n",
    "codes_df = codes_df.join(one_hot_enc_df)\n",
    "\n",
    "sta = time.time()\n",
    "# Merge diagnoses_df and codes_df based on column 'ICD9_CODE'. \n",
    "# Dask operations are lazy and do not materialize until 'compute()' method is invoked.\n",
    "diagnoses_df = diagnoses_df.merge(codes_df, on='ICD9_CODE')\n",
    "end = time.time()\n",
    "print(end - sta)\n",
    "\n",
    "diagnoses_df = diagnoses_df.drop(['ICD9_CODE'], axis = 1)\n",
    "\n",
    "# This step will group all ICD_9 codes corresponding to a given HADM_ID and build a multi-hot embedding.\n",
    "diagnoses_df = diagnoses_df.groupby('HADM_ID').any().reset_index()\n",
    "\n",
    "print(diagnoses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we process MIMIC-III NOTEEVENTS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHS81ejCQulU",
    "outputId": "33e7eb69-4823-4884-93a2-45491bb65d57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       HADM_ID                                               TEXT\n",
      "0       167853  Admission Date:  [**2151-7-16**]       Dischar...\n",
      "1       107527  Admission Date:  [**2118-6-2**]       Discharg...\n",
      "2       167118  Admission Date:  [**2119-5-4**]              D...\n",
      "3       196489  Admission Date:  [**2124-7-21**]              ...\n",
      "4       135453  Admission Date:  [**2162-3-3**]              D...\n",
      "...        ...                                                ...\n",
      "55970   147266  Admission Date:  [**2147-2-25**]              ...\n",
      "55971   129802  Admission Date:  [**2190-5-13**]              ...\n",
      "55972   182558  Admission Date:  [**2121-6-13**]              ...\n",
      "55973   184741  Admission Date:  [**2182-4-19**]              ...\n",
      "55974   121964  Admission Date:  [**2186-6-16**]              ...\n",
      "\n",
      "[55102 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import dataset and pre-process.\n",
    "notes_df = pd.read_csv('drive/My Drive/cs598-dl/data/NOTEEVENTS.csv', usecols=['HADM_ID', \"CATEGORY\",\"DESCRIPTION\", \"TEXT\"])\n",
    "notes_df = notes_df.dropna()\n",
    "\n",
    "# Only filter-in notes which are 'Discharge summary' and are of sub-type 'Report'.\n",
    "notes_df = notes_df[(notes_df['CATEGORY'] == 'Discharge summary') & (notes_df['DESCRIPTION'] == 'Report')]\n",
    "notes_df = notes_df.drop(['CATEGORY', 'DESCRIPTION'], axis=1)\n",
    "notes_df = notes_df.astype({'HADM_ID': 'int64'})\n",
    "notes_df = notes_df.drop_duplicates(subset = 'HADM_ID')\n",
    "print(notes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We next select the subset of rows in diagnoses_df and notes_df with common set of HADM_IDs, \n",
    "# and remove other rows from each DataFrame. Such rows can not be used in training or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HcpXLsjZad0K",
    "outputId": "1da96b16-790e-4e08-9e13-0a68424bb6b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       HADM_ID\n",
      "0       100001\n",
      "1       100003\n",
      "2       100006\n",
      "3       100007\n",
      "4       100009\n",
      "...        ...\n",
      "58971   199993\n",
      "58972   199994\n",
      "58973   199995\n",
      "58974   199998\n",
      "58975   199999\n",
      "\n",
      "[58976 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Collect all hadm_ids from diagnoses_df\n",
    "hadm_ids_from_diagnoses_df = diagnoses_df.filter(items = ['HADM_ID'])\n",
    "\n",
    "# Collect all hadm_ids from diagnoses_df\n",
    "hadm_ids_from_notes_df = notes_df.filter(items = ['HADM_ID'])\n",
    "\n",
    "# Generate DataFrame with common set of HADM_IDs.\n",
    "hadm_ids_df = hadm_ids_from_diagnoses_df.merge(hadm_ids_from_notes_df, how = 'inner')\n",
    "\n",
    "# Filter rows in daignoses_df by merging with DataFrame containing common HADM_IDs.\n",
    "diagnoses_df = diagnoses_df.merge(hadm_ids_df, on='HADM_ID', how = 'right')\n",
    "\n",
    "# Similarly, filter rows in notes_df by merging with DataFrame containing common HADM_IDs.\n",
    "notes_df = notes_df.merge(hadm_ids_df, on='HADM_ID', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUKIuqsLvCvz"
   },
   "outputs": [],
   "source": [
    "# Pickle diagnoses_df\n",
    "diagnoses_df.to_pickle(PROJECT_DIR + 'data/DIAGNOSES.pkl')\n",
    "\n",
    "# Pickle notes_df\n",
    "notes_df.to_pickle(PROJECT_DIR + 'data/NOTES.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_df = Da"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-rapids] *",
   "language": "python",
   "name": "conda-env-.conda-rapids-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
