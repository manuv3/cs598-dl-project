{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb2dba8",
   "metadata": {},
   "source": [
    "# Word2Vec and Doc2Vec Embeddings Generation\n",
    "\n",
    "- Each document can be viewed as a bag of words. Collectively all the words (tokens) across documents constitute the vocabulary. Each of these words will be converted to fixed length dense vector known using Word2Vec algorithm. The size of each vector is 100. \n",
    "\n",
    "\n",
    "- Doc2Vec algorithm produces a dense vector representation of a document. It considers the global ordering of words in the document. We transform each document (discharge summary) to D2V embedding of size 128.\n",
    "\n",
    "\n",
    "- These embeddings D2V (per document) and W2V (per word in vocabulary) are used in our deep learning model as input encodings.\n",
    "\n",
    "- We using **Gensim** provided Word2Vec and Doc2Vec models to train and create embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d707c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.parsing.preprocessing import preprocess_string, split_on_space\n",
    "import random\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c17f603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount the project directory in Google drive. (Its only intended to be run in colab environment.)\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00ed31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base project directory.\n",
    "\n",
    "PROJECT_DIR = 'drive/My Drive/cs598-dl/' # For Google drive only\n",
    "\n",
    "# PROJECT_DIR = '../' # For local directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd93e24",
   "metadata": {},
   "source": [
    "## Word2Vec Model\n",
    "\n",
    "We will start will Word2Vec embedding generation for the words used across discharge summary documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "233aff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Streaming iterator to read discharge summary in a streaming fashion \n",
    "# as list of sentences, each sentence containing list of tokens.\n",
    "# This is needed as input source for Gensim Word2Vec model.\n",
    "# Such a streaming sources enables Gensim to train the model without needing to load the whole corpus in memory.\n",
    "\n",
    "class NotesIter(object):\n",
    "    def __iter__(self):\n",
    "        with open(PROJECT_DIR + 'data/NOTES_2.csv') as notes:\n",
    "            reader = csv.reader(notes)\n",
    "            for row in reader:\n",
    "                break\n",
    "            for row in reader:\n",
    "                report = row[2]\n",
    "                sentences = report.splitlines()\n",
    "                for sentence in sentences:\n",
    "                    yield self._getTokens(sentence)\n",
    "    def _getTokens(self, sentence):\n",
    "        # Here we use Gensim default pre-processing which will tokenize the text with following transformations:\n",
    "        # strip (html) tags,\n",
    "        # strip punctuation,\n",
    "        # strip multiple whitespaces,\n",
    "        # strip numbers,\n",
    "        # remove stopwords,\n",
    "        # strip short words (smaller that 3 characters),\n",
    "        # stem text\n",
    "        return preprocess_string(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a7723ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent: 2212.24529337883\n"
     ]
    }
   ],
   "source": [
    "# Create Word2Vec model with NotesIter as input source, and ouput vector size 100.\n",
    "sta = time.time()\n",
    "model = Word2Vec(NotesIter(), min_count=1, vector_size = 100, workers=4, seed = seed)\n",
    "end = time.time()\n",
    "print('Time spent: ' + str(end - sta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e369bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and save the map (of-sort) of vocabulary tokens and corresponding vectors.\n",
    "# The 'wv' property of the model is KeyedVector object, \n",
    "# which is memory efficient representation of the trained model.\n",
    "\n",
    "wv = model.wv\n",
    "wv.save(PROJECT_DIR + 'data/wv.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e2678",
   "metadata": {},
   "source": [
    "## Doc2Vec Model\n",
    "\n",
    "Now, we create a Doc2Vec model and generate embeddings per discharge summary document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "696c42b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an Iterator which will read document and generate a list of tokens. \n",
    "# This iterator will be used as a streaming source for Doc2Vec model.\n",
    "# Such a streaming sources enables Gensim to train the model without needing to load the whole corpus in memory.\n",
    "\n",
    "class DocsIter(object):\n",
    "    def __iter__(self):\n",
    "        with open(PROJECT_DIR + 'data/NOTES_2.csv') as notes:\n",
    "            reader = csv.reader(notes)\n",
    "            for row in reader:\n",
    "                break\n",
    "            for row in reader:\n",
    "                # Here we tag each document with corresponding HADM_ID (Hospitalization ID).\n",
    "                yield TaggedDocument(words = preprocess_string(row[2]), tags = [row[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d0016ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent: 3078.5452456474304\n"
     ]
    }
   ],
   "source": [
    "# Create Doc2Vec model with DocsIter as input source, and ouput vector size 128.\n",
    "\n",
    "sta = time.time()\n",
    "document_model = Doc2Vec(DocsIter(), vector_size=128, min_count=1, workers=4, seed = seed)\n",
    "end = time.time()\n",
    "print('Time spent: ' + str(end - sta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d8254a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and save the map (of-sort) of HADM_ID tokens and corresponding D2V vectors.\n",
    "# The 'dv' property of the model is KeyedVector object, \n",
    "# which is memory efficient representation of the trained model.\n",
    "\n",
    "dv = document_model.dv\n",
    "dv.save(PROJECT_DIR + 'data/dv.kv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-rapids] *",
   "language": "python",
   "name": "conda-env-.conda-rapids-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
